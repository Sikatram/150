# CSE 150: Intro to AI - Probabilistic Reasoning and Decision Making 

## Lecture 1

### What is AI?

It is an extremely broad field

* Origins of AI are over 2000 years old: [Aristotle](https://en.wikipedia.org/wiki/Aristotle) - [Deductive Reasoning](https://en.wikipedia.org/wiki/Deductive_reasoning) 
* [Bertrand Russell](https://en.wikipedia.org/wiki/Bertrand_Russell) and [Alfred Whitehead](https://en.wikipedia.org/wiki/Alfred_North_Whitehead) published [Principia Mathematica](https://en.wikipedia.org/wiki/Principia_Mathematica)
* Alan Turning showed that any form of mathematical reasoning can be processed by a computer
* [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)) is no longer considered AI

CSE 150 is focused on building systems or agents that can reason about the world by using probability. Build models that leverage probability to make decisions.

[Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl) is known for [**Bayesian Network**](https://en.wikipedia.org/wiki/Bayesian_network)

Before this, there were rule-based system. The Bayesian model is good for uncertainties.

Where are these models applicable?

Our AI needs knowledge and we need some way to represent that knowledge. The AI is trying to infer the likelihood of the disease after observing the symptoms

Swine Flu symptoms -- fever, purple nails. multi organ failure

Common flu cna cause fever
Purple Nails can be done painting
Doctors may ask questions such as, "have you been to a pig farm"

Here we can create a bayesian network. It will be based on probabilities that give us a quantitative self-consistent way of reasoning

A graph can show dependence and codependence

A model for modeling sequences over time - can be used in analyzing text over speech (linear and sequential in time)

(The first word affects the second word, the first AND second word affect the third word, the first, second, AND third word affect the fourth word)

Common themes run through different models

### Core Themes
* Probabilistic modeling of uncertainty
* Learning as optimization
* Knowledge as prediction

### Themes of the course
* Tradeoff between power and tractability
  * Some models get way too big and take way too much time and data. A simpler smaller model may be more reasonable and practical.
* Principles vs. heuristics
  * When are formal models like probability appropriate and when do we choose to use a heuristics

---

## Lecture 2

### Probability in AI

Bayesian View of probability: a belief rather than a count of some event

Probability Theory $==$ "How knowledge affects belief" (Poole and Mackworth)

*Viewing probability as measuring belief (rather than frequency of events) is known as the Bayesian view of probability (as opposed to the frequentest view).*

### Variables and Values

Discrete "random variables", denoted with capital letters: e.g., $X$

Domain of possible values for a variable, denoted with lowercase letter: e.g. $\{x_1, x_2, x_3, \ldots , x_n\}$

Example: $Weather\space W; \space \{w_1 = sunny,\space w_2 = cloudy\}$

### Unconditional (prior) Probability

$P(X = x)$

e.g., What is the probability that the weather is sunny?
$P(W = w_1) = 0.8$

### Axioms of Probability

* $P(X = x) \ge 0$
  * Probabilities are non-negative
* $\displaystyle\sum_{i=1} ^ {n} P(X = x_i) = 1$
  * My belief in all states sum to $1$
* $P(X=x_i\space or\space X=x_j)=P(X=x_i)+P(X=x_j)\space if\space x \ne x_j$
  * You can independently combine the states if they're mutually exclusive

### Conditional Probability

* $P(X = x_i|Y = y_j)$
  * "What is my belief that $X = x_i$ if I already know $Y = y_j$"

Usually, knowing Y gives you information about X, i.e. changes your belief in X. In this case, X and Y are said to be **dependent**.

*i.e.*

$P(X = x_i) \ne P(X=x_i|Y=y_j)$ 

* X and Y are dependent

*e.g.*

$P(W = sunny) = 0.8$

$P(W = sunny | M = june) = 0.6$

### Independence

$P(X = x_i | Y = y_i)=P(X=x_i)$
* Sometimes knowing Y does not change your belief in X. In this case, X and Y are said to be independent.

*e.g.*

$P(X=w_i|Y=y_j)=P(W=w_i)$

Example Question: For which variable Y is the above statement most likely true?

1. Y = The Weather yesterday
2. Y = The day of the week
3. Y = The temperature

### Conditional Independence

